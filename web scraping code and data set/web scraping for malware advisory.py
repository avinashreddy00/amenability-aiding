import urllib.request
from pprint import pprint
from html_table_parser.parser import HTMLTableParser
import pandas as pd
import requests
from bs4 import BeautifulSoup

def url_get_contents(url):
    req = urllib.request.Request(url=url)
    f = urllib.request.urlopen(req)
    return f.read()

url = 'https://www.cisa.gov/known-exploited-vulnerabilities-catalog'

#loading the table

xhtml = url_get_contents(url).decode('utf-8')

p = HTMLTableParser()

p.feed(xhtml)

a=pd.DataFrame(p.tables[0])

#loading the links

reqs = requests.get(url)

soup = BeautifulSoup(reqs.text, 'html.parser')
 
urls = []

for link in soup.find_all('a'):
    if "CVE" in link.get('href'):
        urls.append(link.get('href'))

urls.insert(0, "Reference link")

#adding links to dataframe

for i in range(0, len(urls)):
    a.loc[i, 9] = urls[i]
    
#define function to swap columns
def swap_columns(df, col1, col2):
    col_list = list(df.columns)
    x, y = col_list.index(col1), col_list.index(col2)
    col_list[y], col_list[x] = col_list[x], col_list[y]
    df = df[col_list]
    return df

#swap points and rebounds columns
b = swap_columns(a, 8, 9)

print(b.head(6))

#a.to_csv(r'C:\Users\avina\Desktop\web scraping\my_data.csv', index=False)
# for i in urls:
# #     if "CVE" in i:
#         print(i)
